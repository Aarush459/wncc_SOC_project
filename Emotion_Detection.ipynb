{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P7Bc8P5gRcqR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deWabQzeSET9",
        "outputId": "5b4843de-bc5d-40ed-bb2d-1af9a435e41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('fer2013.csv')\n",
        "print(\"Dataset loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DryicVDCSh8N",
        "outputId": "2f2bc3fd-9e2d-432b-e72f-8a38eb350968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35887 entries, 0 to 35886\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  35887 non-null  int64 \n",
            " 1   pixels   35887 non-null  object\n",
            " 2   Usage    35887 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 841.2+ KB\n",
            "None\n",
            "\n",
            "Emotion distribution:\n",
            " emotion\n",
            "3    8989\n",
            "6    6198\n",
            "4    6077\n",
            "2    5121\n",
            "0    4953\n",
            "5    4002\n",
            "1     547\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Usage distribution:\n",
            " Usage\n",
            "Training       28709\n",
            "PublicTest      3589\n",
            "PrivateTest     3589\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(data.head())\n",
        "print(data.info())\n",
        "print(\"\\nEmotion distribution:\\n\", data['emotion'].value_counts())\n",
        "print(\"\\nUsage distribution:\\n\", data['Usage'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xe8FESzDSwmA"
      },
      "outputs": [],
      "source": [
        "img_size = 48\n",
        "# Convert pixel string to numpy array\n",
        "def preprocess_pixels(pixel_string):\n",
        "    pixels = np.array(pixel_string.split(' '), dtype='float32')\n",
        "    return pixels.reshape(img_size, img_size, 1)\n",
        "\n",
        "data['pixels'] = data['pixels'].apply(preprocess_pixels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4snItlA_T8CE"
      },
      "outputs": [],
      "source": [
        "emotion_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
        "num_classes = len(emotion_labels)\n",
        "# Convert numerical labels to one-hot encoded vectors [1, 7]\n",
        "data['emotion_one_hot'] = data['emotion'].apply(lambda x: to_categorical(x, num_classes=num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA9tIBefU5n8",
        "outputId": "d8bb6666-ed95-4766-c709-f3f1eb69aa3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training data shape: (28709, 48, 48, 1), labels shape: (28709, 7)\n",
            "Validation data shape: (3589, 48, 48, 1), labels shape: (3589, 7)\n",
            "Test data shape: (3589, 48, 48, 1), labels shape: (3589, 7)\n"
          ]
        }
      ],
      "source": [
        "#  Data Splitting\n",
        "train_df = data[data['Usage'] == 'Training']\n",
        "test_df = data[data['Usage'] == 'PublicTest']\n",
        "val_df = data[data['Usage'] == 'PrivateTest']\n",
        "\n",
        "#  features (X) and labels (y) for each set\n",
        "X_train = np.stack(train_df['pixels'].values)\n",
        "y_train = np.stack(train_df['emotion_one_hot'].values)\n",
        "\n",
        "X_val = np.stack(val_df['pixels'].values)\n",
        "y_val = np.stack(val_df['emotion_one_hot'].values)\n",
        "\n",
        "X_test = np.stack(test_df['pixels'].values)\n",
        "y_test = np.stack(test_df['emotion_one_hot'].values)\n",
        "\n",
        "print(f\"\\nTraining data shape: {X_train.shape}, labels shape: {y_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}, labels shape: {y_val.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}, labels shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGY5GW0lYpXT",
        "outputId": "7598f554-9632-4a5e-e878-62eb79403235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = Sequential(name=\"Emotion_Detection_CNN\")\n",
        "\n",
        "# Block 1\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Block 2\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Block 3\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flatten the feature maps to feed into the Dense layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully Connected Layers\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(7, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xlip_w_PaPgs",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y1vb6ybajFT",
        "outputId": "93a550a7-c6be-44e4-aca0-871108299f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Model Training ---\n",
            "Epoch 1/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 691ms/step - accuracy: 0.2828 - loss: 2.2002 - val_accuracy: 0.4227 - val_loss: 1.4889\n",
            "Epoch 2/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 659ms/step - accuracy: 0.4292 - loss: 1.4994 - val_accuracy: 0.4539 - val_loss: 1.4188\n",
            "Epoch 3/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 644ms/step - accuracy: 0.4814 - loss: 1.3630 - val_accuracy: 0.4990 - val_loss: 1.2905\n",
            "Epoch 4/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 643ms/step - accuracy: 0.5108 - loss: 1.2835 - val_accuracy: 0.5118 - val_loss: 1.2762\n",
            "Epoch 5/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 639ms/step - accuracy: 0.5253 - loss: 1.2471 - val_accuracy: 0.5258 - val_loss: 1.2246\n",
            "Epoch 6/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 661ms/step - accuracy: 0.5485 - loss: 1.1911 - val_accuracy: 0.4893 - val_loss: 1.3388\n",
            "Epoch 7/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 658ms/step - accuracy: 0.5610 - loss: 1.1564 - val_accuracy: 0.5425 - val_loss: 1.2139\n",
            "Epoch 8/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 646ms/step - accuracy: 0.5700 - loss: 1.1304 - val_accuracy: 0.5534 - val_loss: 1.2951\n",
            "Epoch 9/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 650ms/step - accuracy: 0.5889 - loss: 1.0968 - val_accuracy: 0.5587 - val_loss: 1.2070\n",
            "Epoch 10/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 650ms/step - accuracy: 0.5974 - loss: 1.0720 - val_accuracy: 0.5687 - val_loss: 1.1247\n",
            "Epoch 11/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 646ms/step - accuracy: 0.6089 - loss: 1.0329 - val_accuracy: 0.5146 - val_loss: 1.2898\n",
            "Epoch 12/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 635ms/step - accuracy: 0.5956 - loss: 1.0639 - val_accuracy: 0.5687 - val_loss: 1.1378\n",
            "Epoch 13/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 637ms/step - accuracy: 0.6294 - loss: 0.9806 - val_accuracy: 0.5862 - val_loss: 1.0788\n",
            "Epoch 14/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 639ms/step - accuracy: 0.6320 - loss: 0.9786 - val_accuracy: 0.5954 - val_loss: 1.0595\n",
            "Epoch 15/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 640ms/step - accuracy: 0.6524 - loss: 0.9327 - val_accuracy: 0.5974 - val_loss: 1.1098\n",
            "Epoch 16/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 653ms/step - accuracy: 0.6570 - loss: 0.9170 - val_accuracy: 0.6007 - val_loss: 1.0800\n",
            "Epoch 17/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 645ms/step - accuracy: 0.6717 - loss: 0.8755 - val_accuracy: 0.5756 - val_loss: 1.1406\n",
            "Epoch 18/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 621ms/step - accuracy: 0.6760 - loss: 0.8606 - val_accuracy: 0.5957 - val_loss: 1.1705\n",
            "Epoch 19/100\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 618ms/step - accuracy: 0.6862 - loss: 0.8337 - val_accuracy: 0.5520 - val_loss: 1.5607\n",
            "Epoch 20/100\n",
            "\u001b[1m159/449\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:45\u001b[0m 572ms/step - accuracy: 0.7002 - loss: 0.8166"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"--- Model Training Complete ---\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4FJ6dK2bTVE"
      },
      "outputs": [],
      "source": [
        "print(\"\\nEvaluating model on the test set...\")\n",
        "test_loss,test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.2f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WauqqP1AbrIV"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Evaluating Model on Public Test Set ---\")\n",
        "public_test_loss, public_test_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Model Accuracy on Public Test Set: {public_test_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEicKvl0efoW"
      },
      "outputs": [],
      "source": [
        "# Select a random index from the validation set\n",
        "random_index = np.random.randint(0, len(X_test))\n",
        "\n",
        "# Get the random image and its one-hot encoded label\n",
        "random_image = X_test[random_index]\n",
        "actual_emotion_one_hot = y_test[random_index]\n",
        "\n",
        "# ✨ FIX: Convert the one-hot encoded array to a single integer label\n",
        "actual_emotion_label = np.argmax(actual_emotion_one_hot)\n",
        "actual_emotion_name = emotion_labels[actual_emotion_label]\n",
        "\n",
        "# Reshape the image to make a batch of size 1 for prediction\n",
        "random_image_for_prediction = np.expand_dims(random_image, axis=0)\n",
        "\n",
        "# Make a prediction\n",
        "predicted_probabilities = model.predict(random_image_for_prediction)\n",
        "predicted_emotion_label = np.argmax(predicted_probabilities)\n",
        "predicted_emotion_name = emotion_labels[predicted_emotion_label]\n",
        "\n",
        "# Display the image and the results\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(random_image.reshape(48, 48), cmap='gray')\n",
        "plt.title(f\"Actual: {actual_emotion_name}\\nPredicted: {predicted_emotion_name}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}